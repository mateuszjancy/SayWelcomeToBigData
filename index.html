<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      @import url(https://fonts.googleapis.com/css?family=Yanone+Kaffeesatz);
      @import url(https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic);
      @import url(https://fonts.googleapis.com/css?family=Ubuntu+Mono:400,700,400italic);

      body { font-family: 'Droid Serif'; }
      h1, h2, h3 {
        font-family: 'Yanone Kaffeesatz';
        font-weight: normal;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
    </style>
  </head>
  <body>
    <textarea id="source">

class: center, middle

<pre>
███████╗ ██████╗ █████╗ ██╗      █████╗
██╔════╝██╔════╝██╔══██╗██║     ██╔══██╗
███████╗██║     ███████║██║     ███████║
╚════██║██║     ██╔══██║██║     ██╔══██║
███████║╚██████╗██║  ██║███████╗██║  ██║
╚══════╝ ╚═════╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝

██╗   ██╗███████╗███████╗██████╗
██║   ██║██╔════╝██╔════╝██╔══██╗
██║   ██║███████╗█████╗  ██████╔╝
██║   ██║╚════██║██╔══╝  ██╔══██╗
╚██████╔╝███████║███████╗██║  ██║
 ╚═════╝ ╚══════╝╚══════╝╚═╝  ╚═╝

 ██████╗ ██████╗  ██████╗ ██╗   ██╗██████╗
██╔════╝ ██╔══██╗██╔═══██╗██║   ██║██╔══██╗
██║  ███╗██████╔╝██║   ██║██║   ██║██████╔╝
██║   ██║██╔══██╗██║   ██║██║   ██║██╔═══╝
╚██████╔╝██║  ██║╚██████╔╝╚██████╔╝██║
 ╚═════╝ ╚═╝  ╚═╝ ╚═════╝  ╚═════╝ ╚═╝
</pre>
---
<pre>
        GGGGGGGGGGGGGFFFFFFFFFFFFFFFFFFFFFFTTTTTTTTTTTTTTTTTTTTTTT
     GGG::::::::::::GF::::::::::::::::::::FT:::::::::::::::::::::T
   GG:::::::::::::::GF::::::::::::::::::::FT:::::::::::::::::::::T
  G:::::GGGGGGGG::::GFF::::::FFFFFFFFF::::FT:::::TT:::::::TT:::::T
 G:::::G       GGGGGG  F:::::F       FFFFFFTTTTTT  T:::::T  TTTTTT
G:::::G                F:::::F                     T:::::T
G:::::G                F::::::FFFFFFFFFF           T:::::T
G:::::G    GGGGGGGGGG  F:::::::::::::::F           T:::::T
G:::::G    G::::::::G  F:::::::::::::::F           T:::::T
G:::::G    GGGGG::::G  F::::::FFFFFFFFFF           T:::::T
G:::::G        G::::G  F:::::F                     T:::::T
 G:::::G       G::::G  F:::::F                     T:::::T
  G:::::GGGGGGGG::::GFF:::::::FF                 TT:::::::TT
   GG:::::::::::::::GF::::::::FF                 T:::::::::T
     GGG::::::GGG:::GF::::::::FF                 T:::::::::T
        GGGGGG   GGGGFFFFFFFFFFF                 TTTTTTTTTTT
</pre>
---
<pre>
███╗   ███╗ █████╗ ████████╗███████╗██╗   ██╗███████╗███████╗
████╗ ████║██╔══██╗╚══██╔══╝██╔════╝██║   ██║██╔════╝╚══███╔╝
██╔████╔██║███████║   ██║   █████╗  ██║   ██║███████╗  ███╔╝
██║╚██╔╝██║██╔══██║   ██║   ██╔══╝  ██║   ██║╚════██║ ███╔╝
██║ ╚═╝ ██║██║  ██║   ██║   ███████╗╚██████╔╝███████║███████╗
╚═╝     ╚═╝╚═╝  ╚═╝   ╚═╝   ╚══════╝ ╚═════╝ ╚══════╝╚══════╝

     ██╗ █████╗ ███╗   ██╗ ██████╗██╗   ██╗
     ██║██╔══██╗████╗  ██║██╔════╝╚██╗ ██╔╝
     ██║███████║██╔██╗ ██║██║      ╚████╔╝
██   ██║██╔══██║██║╚██╗██║██║       ╚██╔╝
╚█████╔╝██║  ██║██║ ╚████║╚██████╗   ██║
</pre>
---
<pre>
#     _____               _          _ _            
#    / ____|             | |        | | |           
#   | (___   __ _ _   _  | |__   ___| | | ___       
#    \___ \ / _` | | | | | '_ \ / _ \ | |/ _ \      
#    ____) | (_| | |_| | | | | |  __/ | | (_) |     
#   |_____/ \__,_|\__, | |_| |_|\___|_|_|\___/      
#                  __/ |                            
#    _          __|___/       _____        _        
#   | |        |  _ \(_)     |  __ \      | |       
#   | |_ ___   | |_) |_  __ _| |  | | __ _| |_ __ _ 
#   | __/ _ \  |  _ <| |/ _` | |  | |/ _` | __/ _` |
#   | || (_) | | |_) | | (_| | |__| | (_| | || (_| |
#    \__\___/  |____/|_|\__, |_____/ \__,_|\__\__,_|
#                        __/ |                      
#                       |___/                       
</pre>

---

#Lets define BigData#
* Wiki: [Big data is a term for data sets that are so large or complex that traditional data processing applications are inadequate](https://en.wikipedia.org/wiki/Big_data)
* Experience: Piece of data which is too big to fit into any reasonable hardware with Oracle installation

---

#Key components#
<pre>
#    _    _           _                   
#   | |  | |         | |                  
#   | |__| | __ _  __| | ___   ___  _ __  
#   |  __  |/ _` |/ _` |/ _ \ / _ \| '_ \ 
#   | |  | | (_| | (_| | (_) | (_) | |_) |
#   |_|  |_|\__,_|\__,_|\___/ \___/| .__/ 
#                                  | |    
#                                  |_|    
</pre>

---

##The Hadoop Distributed File System (HDFS) is:##
* **Distributed** file system designed to run on commodity hardware. 
* Highly **fault-tolerant**
* Designed to be deployed on **low-cost hardware**
* Provides high throughput **access** to application data and is suitable for **applications that have large data sets**.
* [Hdfs design](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html)

---

##Apache Hadoop YARN##
The fundamental idea of YARN is to **split up** the functionalities of **resource management** and **job scheduling/monitoring** into separate daemons.
* [YARN](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html)

---

<pre>
#    _    _ _           
#   | |  | (_)          
#   | |__| |___   _____ 
#   |  __  | \ \ / / _ \
#   | |  | | |\ V /  __/
#   |_|  |_|_| \_/ \___|
#                       
#                       
</pre>

The **Apache Hive** data warehouse software facilitates **querying** and **managing** large datasets residing in distributed storage. Hive provides a mechanism to **project structure** onto this data and **query the data using a SQL-like language** called HiveQL.
* [Hive Architecture](https://cwiki.apache.org/confluence/display/Hive/Design)

---

##DDL Operations##
```sql
  CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING)
  ALTER TABLE pokes ADD COLUMNS (new_col INT)
```
##DML Operations##
```sql
LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes;
-- Loads a file that contains two columns separated by ctrl-a into pokes table. 'LOCAL' signifies that the input file is on the local file system. If 'LOCAL' is omitted then it looks for the file in HDFS.
-- The keyword 'OVERWRITE' signifies that existing data in the table is deleted. If the 'OVERWRITE' keyword is omitted, data files are appended to existing data sets.
```
##SQL Operations##
```sql
SELECT a.foo FROM invites a WHERE a.ds='2008-08-15';
```

---

<pre>
#     _____                  _    
#    / ____|                | |   
#   | (___  _ __   __ _ _ __| | __
#    \___ \| '_ \ / _` | '__| |/ /
#    ____) | |_) | (_| | |  |   < 
#   |_____/| .__/ \__,_|_|  |_|\_\
#          | |                    
#          |_|                    
</pre>
**Apache Spark is a fast and general engine for large-scale data processing.**

---

#Cluster Mode Overview#
![Sparck clueste architecture](http://spark.apache.org/docs/latest/img/cluster-overview.png)

---

#Cluster Manager Types#
* Standalone – a simple cluster manager included with Spark that makes it easy to set up a cluster.
* Apache Mesos – a general cluster manager that can also run Hadoop MapReduce and service applications.
* Hadoop YARN – the resource manager in Hadoop 2.

---

#Submitting Applications#
```
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8] \
  /path/to/examples.jar \
```

---

#Interactive Analysis with the Spark Shell#
Spark’s shell provides a **simple way to learn the API**, as well as a powerful tool to analyze data interactively.

```
./bin/spark-shell
```

---

#Self-Contained Applications#
###SBT###
```scala
name := "Simple Project"

version := "1.0"

scalaVersion := "2.10.5"

libraryDependencies += "org.apache.spark" %% "spark-core" % "1.6.0"
```

---

###Job###
```scala
/* SimpleApp.scala */
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

object SimpleApp {
  def main(args: Array[String]) {
    val logFile = "YOUR_SPARK_HOME/README.md" // Should be some file on your system
    val conf = new SparkConf().setAppName("Simple Application")
    val sc = new SparkContext(conf)
    val logData = sc.textFile(logFile, 2).cache()
    val numAs = logData.filter(line => line.contains("a")).count()
    val numBs = logData.filter(line => line.contains("b")).count()
    println("Lines with a: %s, Lines with b: %s".format(numAs, numBs))
  }
}
```

---

#RDD#
Spark revolves around the concept of a **resilient distributed dataset (RDD)**, which is a **fault-tolerant collection of elements that can be operated on in parallel**. There are two ways to create RDDs: **parallelizing an existing collection** in your driver program, or **referencing a dataset in an external storage system**, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.

RDDs support two types of operations: **transformations**, which create a new dataset from an existing one, and **actions**, which return a value to the driver program after running a computation on the dataset

By default, each transformed **RDD may be recomputed each time you run an action on it**. However, you may also persist an RDD in memory using the **persist (or cache) method**, in which case Spark **will keep the elements around on the cluster** for much **faster access the next time** you query it. 

```scala
val lines = sc.textFile("data.txt")
val lineLengths = lines.map(s => s.length)
val totalLength = lineLengths.reduce((a, b) => a + b)
```

---

#Spark SQL, DataFrames#
* Spark SQL is a Spark module for structured data processing.
* One use of Spark SQL is to execute SQL queries written using either a basic SQL syntax or HiveQL. 
* Spark SQL can also be used to read data from an existing Hive installation.
* A DataFrame is a distributed collection of data organized into named columns.


###Getting Started###
```scala
val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._
```

---

###DataFrame Operations###
```scala
val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// Create the DataFrame
val df = sqlContext.read.json("examples/src/main/resources/people.json")
```

---
#Show the content of the DataFrame#
```scala
df.show()
// age  name
// null Michael
// 30   Andy
// 19   Justin
```

---

#Print the schema in a tree format#
```scala
df.printSchema()
// root
// |-- age: long (nullable = true)
// |-- name: string (nullable = true)
```

---

#Select only the "name" column#
```scala
df.select("name").show()
// name
// Michael
// Andy
// Justin
```

---

#Select everybody, but increment the age by 1#
```scala
df.select(df("name"), df("age") + 1).show()
// name    (age + 1)
// Michael null
// Andy    31
// Justin  20

```

---

#Select people older than 21#
```scala
df.filter(df("age") > 21).show()
// age name
// 30  Andy
```

---

#Count people by age#
```scala
df.groupBy("age").count().show()
// age  count
// null 1
// 19   1
// 30   1
```

###Running SQL Queries Programmatically###
```scala
val sqlContext = ... // An existing SQLContext
val df = sqlContext.sql("SELECT * FROM table")
```

###DataFrame Operations###
```scala
val sc: SparkContext // An existing SparkContext.
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._

// Create the DataFrame
val df = sqlContext.read.json("examples/src/main/resources/people.json")

// Show the content of the DataFrame
df.show()
// age  name
// null Michael
// 30   Andy
// 19   Justin

// Print the schema in a tree format
df.printSchema()
// root
// |-- age: long (nullable = true)
// |-- name: string (nullable = true)

// Select only the "name" column
df.select("name").show()
// name
// Michael
// Andy
// Justin

// Select everybody, but increment the age by 1
df.select(df("name"), df("age") + 1).show()
// name    (age + 1)
// Michael null
// Andy    31
// Justin  20

// Select people older than 21
df.filter(df("age") > 21).show()
// age name
// 30  Andy

// Count people by age
df.groupBy("age").count().show()
// age  count
// null 1
// 19   1
// 30   1
```

###Running SQL Queries Programmatically###
```scala
val sqlContext = ... // An existing SQLContext
val df = sqlContext.sql("SELECT * FROM table")
```

---

#Understanding closures#
```scala
var counter = 0
var rdd = sc.parallelize(data)

// Wrong: Don't do this!!
rdd.foreach(x => counter += x)

println("Counter value: " + counter)
```

---
#Shared Variables#
###Broadcast Variables###
Allow the programmer **to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.** They can be used, for example, to give every node a **copy of a large input dataset in an efficient manner**. 

```scala
scala> val broadcastVar = sc.broadcast(Array(1, 2, 3))
broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)

scala> broadcastVar.value
res0: Array[Int] = Array(1, 2, 3)
```

---
#Shared Variables#
###Accumulators## 
Are variables that are **only “added”** to through an associative operation and can therefore be efficiently **supported in parallel**. They can be used **to implement counters** (as in MapReduce) or sums. 

```scala
scala> val accum = sc.accumulator(0, "My Accumulator")
accum: spark.Accumulator[Int] = 0

scala> sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
...
10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s

scala> accum.value
res2: Int = 10
```

---

#And many others#
* Impala
* Flume
* Kite
* ...

---

#Key players#
* [Cloudera](https://cloudera.com/)
* [Hortonworks](http://hortonworks.com/)
* [Databricks](https://databricks.com/)
* [MapR](https://www.mapr.com/)


---
<pre>
#     ____                  
#    / __ \  ___      /\    
#   | |  | |( _ )    /  \   
#   | |  | |/ _ \/\ / /\ \  
#   | |__| | (_>  </ ____ \ 
#    \___\_\\___/\/_/    \_\
#                           
#                                                              
</pre>
    </textarea>
    <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js">
    </script>
    <script>
      var slideshow = remark.create();
    </script>
  </body>
</html>
